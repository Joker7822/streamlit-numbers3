import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor
from sklearn.ensemble import (
    GradientBoostingRegressor, RandomForestRegressor, StackingRegressor,
    GradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingClassifier,
    AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier
)
from sklearn.linear_model import Ridge, LogisticRegression, RidgeClassifier, Perceptron, PassiveAggressiveClassifier, SGDClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.neural_network import MLPRegressor
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.ensemble import StackingRegressor
from statsmodels.tsa.arima.model import ARIMA
from stable_baselines3 import PPO
from gym import spaces
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import optuna
import matplotlib
matplotlib.use('Agg')  # â† â˜… ã“ã®è¡Œã‚’å…ˆã«è¿½åŠ ï¼
import matplotlib.pyplot as plt
import aiohttp
import asyncio
import warnings
import re
import platform
import gym
import sys
import os
import random
from sklearn.metrics import precision_score, recall_score, f1_score
from neuralforecast.models import TFT
from neuralforecast import NeuralForecast
import onnxruntime
import streamlit as st
from autogluon.tabular import TabularPredictor
import torch.backends.cudnn
from datetime import datetime 
from collections import Counter
import torch.nn.functional as F
import math

# Windowsç’°å¢ƒã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãƒãƒªã‚·ãƒ¼ã‚’è¨­å®š
if platform.system() == "Windows":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

warnings.filterwarnings("ignore")

class LotoEnv(gym.Env):
    def __init__(self, historical_numbers):
        super(LotoEnv, self).__init__()
        self.historical_numbers = historical_numbers
        self.action_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)

    def reset(self):
        return np.zeros(10, dtype=np.float32)

    def step(self, action):
        action = np.clip(action, 0, 1)
        selected_numbers = list(np.argsort(action)[-4:])
        winning_numbers = list(np.random.choice(self.historical_numbers, 4, replace=False))

        prize = classify_numbers3_prize(selected_numbers, winning_numbers)
        prize_rewards = {
            "ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ": 468700,
            "ãƒœãƒƒã‚¯ã‚¹": 18700,
            "ã¯ãšã‚Œ": -100
        }

        reward = prize_rewards.get(prize, -100)
        done = True
        obs = np.zeros(10, dtype=np.float32)
        return obs, reward, done, {}

class DiversityEnv(gym.Env):
    def __init__(self, historical_numbers):
        super(DiversityEnv, self).__init__()
        self.historical_numbers = historical_numbers
        self.action_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        self.previous_outputs = set()

    def reset(self):
        return np.zeros(10, dtype=np.float32)

    def step(self, action):
        action = np.clip(action, 0, 1)
        selected = tuple(sorted(np.argsort(action)[-4:]))
        reward = 1.0 if selected not in self.previous_outputs else -1.0
        self.previous_outputs.add(selected)
        return np.zeros(10, dtype=np.float32), reward, True, {}

class CycleEnv(gym.Env):
    def __init__(self, historical_numbers):
        super(CycleEnv, self).__init__()
        self.historical_numbers = historical_numbers
        self.action_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        self.cycle_scores = calculate_number_cycle_score(historical_numbers)

    def reset(self):
        return np.zeros(10, dtype=np.float32)

    def step(self, action):
        action = np.clip(action, 0, 1)
        selected = np.argsort(action)[-4:]
        avg_cycle = np.mean([self.cycle_scores.get(n, 999) for n in selected])
        reward = max(0, 1 - (avg_cycle / 50))
        return np.zeros(10, dtype=np.float32), reward, True, {}

class MultiAgentPPOTrainer:
    def __init__(self, historical_data, total_timesteps=5000):
        self.historical_data = historical_data
        self.total_timesteps = total_timesteps
        self.agents = {}

    def train_agents(self):
        envs = {
            "accuracy": LotoEnv(self.historical_data),
            "diversity": DiversityEnv(self.historical_data),
            "cycle": CycleEnv(self.historical_data)
        }

        for name, env in envs.items():
            model = PPO("MlpPolicy", env, verbose=0)
            model.learn(total_timesteps=self.total_timesteps)
            self.agents[name] = model
            print(f"[INFO] PPO {name} ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’å®Œäº†")

    def predict_all(self, num_candidates=50):
        predictions = []
        for name, model in self.agents.items():
            obs = model.env.reset()
            for _ in range(num_candidates // 3):
                action, _ = model.predict(obs)
                selected = list(np.argsort(action)[-4:])
                predictions.append((selected, 0.9))  # ä¿¡é ¼åº¦ã¯ä»®
        return predictions

class AdversarialLotoEnv(gym.Env):
    def __init__(self, target_numbers_list):
        """
        GANãŒç”Ÿæˆã—ãŸç•ªå·ï¼ˆtarget_numbers_listï¼‰ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã—ã€
        PPOã«ã€Œãã‚Œã‚‰ã‚’å½“ã¦ã•ã›ã‚‹ã€å¯¾æˆ¦ç’°å¢ƒ
        """
        super(AdversarialLotoEnv, self).__init__()
        self.target_numbers_list = target_numbers_list
        self.current_index = 0
        self.action_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)

    def reset(self):
        self.current_index = (self.current_index + 1) % len(self.target_numbers_list)
        return np.zeros(10, dtype=np.float32)

    def step(self, action):
        action = np.clip(action, 0, 1)
        selected_numbers = set(np.argsort(action)[-3:])
        target_numbers = set(self.target_numbers_list[self.current_index])

        main_match = len(selected_numbers & target_numbers)
        reward = main_match / 3  # éƒ¨åˆ†ä¸€è‡´ç‡ã‚’å ±é…¬ã¨ã™ã‚‹

        done = True
        obs = np.zeros(10, dtype=np.float32)
        return obs, reward, done, {}

def score_real_structure_similarity(numbers):
    """
    æ•°å­—ãƒªã‚¹ãƒˆã«å¯¾ã—ã¦ã€ã€Œæœ¬ç‰©ã‚‰ã—ã„æ§‹é€ ã‹ã©ã†ã‹ã€ã‚’è©•ä¾¡ã™ã‚‹ã‚¹ã‚³ã‚¢ï¼ˆ0ã€œ1ï¼‰
    - åˆè¨ˆãŒ10ã€œ20
    - é‡è¤‡ãŒãªã„
    - ä¸¦ã³ãŒæ˜‡é † or é™é †
    """
    if len(numbers) != 3:
        return 0
    score = 0
    if 10 <= sum(numbers) <= 20:
        score += 1
    if len(set(numbers)) == 3:
        score += 1
    if numbers == sorted(numbers) or numbers == sorted(numbers, reverse=True):
        score += 1
    return score / 3  # æœ€å¤§3ç‚¹æº€ç‚¹ã‚’0ã€œ1ã‚¹ã‚±ãƒ¼ãƒ«

class LotoGAN(nn.Module):
    def __init__(self, noise_dim=100):
        super(LotoGAN, self).__init__()
        self.generator = nn.Sequential(
            nn.Linear(noise_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 10),
            nn.Sigmoid()
        )
        self.discriminator = nn.Sequential(
            nn.Linear(10, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        self.noise_dim = noise_dim

    def generate_samples(self, num_samples):
        noise = torch.randn(num_samples, self.noise_dim)
        with torch.no_grad():
            samples = self.generator(noise)
        return samples.numpy()

    def evaluate_generated_numbers(self, sample_tensor):
        """
        sample_tensor: shape=(10,) ã®Tensorï¼ˆ0ã€œ1å€¤ã§å„æ•°å­—ã®ã‚¹ã‚³ã‚¢ï¼‰
        ä¸Šä½3ã¤ã‚’é¸ã‚“ã§ç•ªå·ã«å¤‰æ› â†’ åˆ¤åˆ¥å™¨ã‚¹ã‚³ã‚¢ã¨æ§‹é€ ã‚¹ã‚³ã‚¢ã‚’åˆæˆ
        """
        numbers = list(np.argsort(sample_tensor.cpu().numpy())[-3:])
        numbers.sort()
        real_score = score_real_structure_similarity(numbers)

        with torch.no_grad():
            discriminator_score = self.discriminator(sample_tensor.unsqueeze(0)).item()

        final_score = 0.5 * discriminator_score + 0.5 * real_score
        return final_score

class DiffusionNumberGenerator(nn.Module):
    def __init__(self, noise_dim=16, steps=100):
        super(DiffusionNumberGenerator, self).__init__()
        self.noise_dim = noise_dim
        self.steps = steps

        self.model = nn.Sequential(
            nn.Linear(noise_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 10),  # å„æ•°å­—ã®ã‚¹ã‚³ã‚¢ï¼ˆ0ã€œ9ï¼‰
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

    def generate(self, num_samples=20):
        samples = []
        for _ in range(num_samples):
            noise = torch.randn(1, self.noise_dim)
            x = noise
            for _ in range(self.steps):
                noise_grad = torch.randn_like(x) * 0.1
                x = x - 0.01 * x + noise_grad
            with torch.no_grad():
                scores = self.forward(x).squeeze().numpy()
            top4 = np.argsort(scores)[-4:]
            samples.append(sorted(top4.tolist()))
        return samples

def create_advanced_features(dataframe):
    dataframe = dataframe.copy()
    def convert_to_number_list(x):
        if isinstance(x, str):
            cleaned = x.strip("[]").replace(",", " ").replace("'", "").replace('"', "")
            return [int(n) for n in cleaned.split() if n.isdigit()]
        return x if isinstance(x, list) else [0]

    dataframe['æœ¬æ•°å­—'] = dataframe['æœ¬æ•°å­—'].apply(convert_to_number_list)
    dataframe['æŠ½ã›ã‚“æ—¥'] = pd.to_datetime(dataframe['æŠ½ã›ã‚“æ—¥'])

    valid_mask = (dataframe['æœ¬æ•°å­—'].apply(len) == 3)
    dataframe = dataframe[valid_mask].copy()

    if dataframe.empty:
        print("[ERROR] æœ‰åŠ¹ãªæœ¬æ•°å­—ãŒå­˜åœ¨ã—ã¾ã›ã‚“ï¼ˆ4æ¡ãƒ‡ãƒ¼ã‚¿ãŒãªã„ï¼‰")
        return pd.DataFrame()  # ç©ºã®DataFrameã‚’è¿”ã™

    nums_array = np.vstack(dataframe['æœ¬æ•°å­—'].values)
    features = pd.DataFrame(index=dataframe.index)

    features['æ•°å­—åˆè¨ˆ'] = nums_array.sum(axis=1)
    features['æ•°å­—å¹³å‡'] = nums_array.mean(axis=1)
    features['æœ€å¤§'] = nums_array.max(axis=1)
    features['æœ€å°'] = nums_array.min(axis=1)
    features['æ¨™æº–åå·®'] = np.std(nums_array, axis=1)

    return pd.concat([dataframe, features], axis=1)

def preprocess_data(data):
    """ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†: ç‰¹å¾´é‡ã®ä½œæˆ & ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°"""
    
    # ç‰¹å¾´é‡ä½œæˆ
    processed_data = create_advanced_features(data)

    if processed_data.empty:
        print("ã‚¨ãƒ©ãƒ¼: ç‰¹å¾´é‡ç”Ÿæˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return None, None, None

    print("=== ç‰¹å¾´é‡ä½œæˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ ===")
    print(processed_data.head())

    # æ•°å€¤ç‰¹å¾´é‡ã®é¸æŠ
    numeric_features = processed_data.select_dtypes(include=[np.number]).columns
    X = processed_data[numeric_features].fillna(0)  # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹

    print(f"æ•°å€¤ç‰¹å¾´é‡ã®æ•°: {len(numeric_features)}, ã‚µãƒ³ãƒ—ãƒ«æ•°: {X.shape[0]}")

    if X.empty:
        print("ã‚¨ãƒ©ãƒ¼: æ•°å€¤ç‰¹å¾´é‡ãŒä½œæˆã•ã‚Œãšã€ãƒ‡ãƒ¼ã‚¿ãŒç©ºã«ãªã£ã¦ã„ã¾ã™ã€‚")
        return None, None, None

    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)

    print("=== ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ ===")
    print(X_scaled[:5])  # æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º

    # ç›®æ¨™å¤‰æ•°ã®æº–å‚™
    try:
        y = np.array([list(map(int, nums)) for nums in processed_data['æœ¬æ•°å­—']])
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: ç›®æ¨™å¤‰æ•°ã®ä½œæˆæ™‚ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None, None, None

    return X_scaled, y, scaler

def convert_numbers_to_binary_vectors(data):
    vectors = []
    for numbers in data['æœ¬æ•°å­—']:
        vec = np.zeros(10)
        for n in numbers:
            if 0 <= n <= 9:
                vec[n] = 1
        vectors.append(vec)
    return np.array(vectors)

def calculate_prediction_errors(predictions, actual_numbers):
    """äºˆæ¸¬å€¤ã¨å®Ÿéš›ã®å½“é¸çµæœã®èª¤å·®ã‚’è¨ˆç®—ã—ã€ç‰¹å¾´é‡ã¨ã—ã¦ä¿å­˜"""
    errors = []
    for pred, actual in zip(predictions, actual_numbers):
        pred_numbers = set(pred[0])
        actual_numbers = set(actual)
        error_count = len(actual_numbers - pred_numbers)
        errors.append(error_count)
    
    return np.mean(errors)

def delete_old_generation_files(directory, days=1):
    """æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€å†…ã§ã€æŒ‡å®šæ—¥æ•°ã‚ˆã‚Šå¤ã„CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
    now = datetime.now()
    deleted = 0
    for filename in os.listdir(directory):
        filepath = os.path.join(directory, filename)
        if os.path.isfile(filepath) and filename.endswith(".csv"):
            try:
                modified_time = datetime.fromtimestamp(os.path.getmtime(filepath))
                if (now - modified_time).days >= days:
                    os.remove(filepath)
                    deleted += 1
            except Exception as e:
                print(f"[WARNING] ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {filename} â†’ {e}")
    if deleted:
        print(f"[INFO] {deleted} ä»¶ã®å¤ã„ä¸–ä»£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")

def save_self_predictions(predictions, file_path="self_predictions.csv", max_records=100, historical_data=None):
    """äºˆæ¸¬çµæœã‚’CSVã«ä¿å­˜ã—ã€ä¸€è‡´æ•°ã¨ç­‰ç´šã‚‚è¨˜éŒ²"""
    rows = []
    valid_grades = ["ã¯ãšã‚Œ", "ãƒœãƒƒã‚¯ã‚¹", "ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ"]  
    for numbers, confidence in predictions:
        match_count = "-"
        prize = "-"
        if historical_data is not None:
            actual_list = [parse_number_string(x) for x in historical_data['æœ¬æ•°å­—'].tolist()]
            match_count = max(len(set(numbers) & set(actual)) for actual in actual_list)
            prize = max(
                (classify_numbers3_prize(numbers, actual) for actual in actual_list),
                key=lambda p: valid_grades.index(p) if p in valid_grades else -1
            )
        rows.append(numbers + [confidence, match_count, prize])

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€ä¸­èº«ãŒç©ºã§ãªã„å ´åˆã®ã¿èª­ã¿è¾¼ã‚€
    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
        try:
            existing = pd.read_csv(file_path, header=None).values.tolist()
            rows = existing + rows
        except pd.errors.EmptyDataError:
            print(f"[WARNING] {file_path} ã¯ç©ºã®ãŸã‚ã€èª­ã¿è¾¼ã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    else:
        print(f"[INFO] {file_path} ãŒç©ºã‹å­˜åœ¨ã—ãªã„ãŸã‚ã€æ–°è¦ä½œæˆã—ã¾ã™ã€‚")

    # æœ€æ–° max_records ä»¶ã«åˆ¶é™ã—ã¦ä¿å­˜
    rows = rows[-max_records:]
    df = pd.DataFrame(rows)
    df.to_csv(file_path, index=False, header=False)
    print(f"[INFO] è‡ªå·±äºˆæ¸¬ã‚’ {file_path} ã«ä¿å­˜ï¼ˆæœ€å¤§{max_records}ä»¶ï¼‰")

    # ğŸ” ä¸–ä»£åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
    gen_dir = "self_predictions_gen"
    os.makedirs(gen_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    generation_file = os.path.join(gen_dir, f"self_predictions_gen_{timestamp}.csv")
    df.to_csv(generation_file, index=False, header=False)
    print(f"[INFO] ä¸–ä»£åˆ¥äºˆæ¸¬ã‚’ä¿å­˜: {generation_file}")

    # ğŸ§¹ å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•å‰Šé™¤ï¼ˆ1æ—¥ä»¥ä¸Šå‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼‰
    delete_old_generation_files(gen_dir, days=1)

def load_self_predictions(file_path="self_predictions.csv", min_match_threshold=3, true_data=None):
    if not os.path.exists(file_path):
        print(f"[INFO] è‡ªå·±äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    if os.path.getsize(file_path) == 0:
        print(f"[INFO] è‡ªå·±äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ã¯ç©ºã§ã™ã€‚")
        return None

    try:
        df = pd.read_csv(file_path, header=None).dropna()

        # 4æ¡äºˆæ¸¬ç•ªå·ã®åˆ—ã‚’å–ã‚Šå‡ºã—
        number_cols = df.iloc[:, :4].astype(int)
        numbers_list = number_cols.values.tolist()

        if true_data is not None:
            scores = evaluate_self_predictions(numbers_list, true_data)

            # âœ… é«˜ä¸€è‡´æ•°ã®ã¿ã‚’åé›†
            valid_predictions = []
            for pred, match in zip(numbers_list, scores):
                if match >= min_match_threshold:
                    valid_predictions.append(tuple(pred))

            # âœ… å‡ºç¾é »åº¦ã¤ãã§è¿”ã™
            from collections import Counter
            freq = Counter(valid_predictions)
            sorted_preds = sorted(freq.items(), key=lambda x: -x[1])  # å›æ•°é™é †

            print(f"[INFO] ä¸€è‡´æ•°{min_match_threshold}ä»¥ä¸Šã®è‡ªå·±äºˆæ¸¬ï¼ˆé‡è¤‡é›†è¨ˆã‚ã‚Šï¼‰: {len(sorted_preds)}ä»¶")
            return sorted_preds  # â† [(ç•ªå·ã‚¿ãƒ—ãƒ«, å‡ºç¾å›æ•°)] å½¢å¼

        return numbers_list

    except Exception as e:
        print(f"[ERROR] è‡ªå·±äºˆæ¸¬èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def evaluate_self_predictions(self_predictions, true_data):
    """
    è‡ªå·±äºˆæ¸¬ãƒªã‚¹ãƒˆã¨æœ¬ç‰©ãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒã—ã¦ä¸€è‡´æ•°ã‚’è©•ä¾¡
    :param self_predictions: [[5,12,17,22,30,34,37], ...]
    :param true_data: éå»ã®æœ¬ç‰©æœ¬æ•°å­—ãƒ‡ãƒ¼ã‚¿ï¼ˆdata['æœ¬æ•°å­—'].tolist()ï¼‰
    :return: å„è‡ªå·±äºˆæ¸¬ã«å¯¾å¿œã™ã‚‹æœ€å¤§ä¸€è‡´æ•°ãƒªã‚¹ãƒˆ
    """
    scores = []
    true_sets = [set(nums) for nums in true_data]

    for pred in self_predictions:
        pred_set = set(pred)
        max_match = 0
        for true_set in true_sets:
            match = len(pred_set & true_set)
            if match > max_match:
                max_match = match
        scores.append(max_match)

    return scores

def update_features_based_on_results(data, accuracy_results):
    """éå»ã®äºˆæ¸¬çµæœã¨å®Ÿéš›ã®çµæœã®æ¯”è¼ƒã‹ã‚‰ç‰¹å¾´é‡ã‚’æ›´æ–°"""
    
    for result in accuracy_results:
        event_date = result["æŠ½ã›ã‚“æ—¥"]
        max_matches = result["æœ€é«˜ä¸€è‡´æ•°"]
        avg_matches = result["å¹³å‡ä¸€è‡´æ•°"]
        confidence_avg = result["ä¿¡é ¼åº¦å¹³å‡"]

        # éå»ã®ãƒ‡ãƒ¼ã‚¿ã«äºˆæ¸¬ç²¾åº¦ã‚’çµ„ã¿è¾¼ã‚€
        data.loc[data["æŠ½ã›ã‚“æ—¥"] == event_date, "éå»ã®æœ€å¤§ä¸€è‡´æ•°"] = max_matches
        data.loc[data["æŠ½ã›ã‚“æ—¥"] == event_date, "éå»ã®å¹³å‡ä¸€è‡´æ•°"] = avg_matches
        data.loc[data["æŠ½ã›ã‚“æ—¥"] == event_date, "éå»ã®äºˆæ¸¬ä¿¡é ¼åº¦"] = confidence_avg

    # ç‰¹å¾´é‡ãŒãªã„å ´åˆã¯0ã§åŸ‹ã‚ã‚‹
    data["éå»ã®æœ€å¤§ä¸€è‡´æ•°"] = data["éå»ã®æœ€å¤§ä¸€è‡´æ•°"].fillna(0)
    data["éå»ã®å¹³å‡ä¸€è‡´æ•°"] = data["éå»ã®å¹³å‡ä¸€è‡´æ•°"].fillna(0)
    data["éå»ã®äºˆæ¸¬ä¿¡é ¼åº¦"] = data["éå»ã®äºˆæ¸¬ä¿¡é ¼åº¦"].fillna(0)

    return data

class LotoLSTM(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(LotoLSTM, self).__init__()
        self.lstm = nn.GRU(input_size, hidden_size, batch_first=True, bidirectional=True)
        self.attn = nn.Linear(hidden_size * 2, 1)
        self.fc = nn.ModuleList([
            nn.Linear(hidden_size * 2, 10) for _ in range(3)  # å„æ¡ï¼š0ã€œ9åˆ†é¡
        ])

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        attn_weights = torch.softmax(self.attn(lstm_out).squeeze(-1), dim=1)
        context = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)
        return [fc(context) for fc in self.fc]  # å„æ¡ã®å‡ºåŠ›

def train_lstm_model(X_train, y_train, input_size, device):
    
    torch.backends.cudnn.benchmark = True  # â˜…ã“ã‚Œã‚’è¿½åŠ 
    
    model = LotoLSTM(input_size=input_size, hidden_size=128).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    dataset = TensorDataset(
        torch.tensor(X_train, dtype=torch.float32),
        torch.tensor(y_train, dtype=torch.float32)
    )
    loader = DataLoader(dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=2)  # â˜…å¤‰æ›´

    scaler = torch.cuda.amp.GradScaler()  # â˜…Mixed Precisionè¿½åŠ 

    model.train()
    for epoch in range(50):
        total_loss = 0
        for batch_X, batch_y in loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            optimizer.zero_grad()
            with torch.cuda.amp.autocast():  # â˜…ã“ã“ã‚‚Mixed Precision
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            total_loss += loss.item()
        print(f"[LSTM] Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}")

    # ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    dummy_input = torch.randn(1, 1, input_size).to(device)
    torch.onnx.export(
        model,
        dummy_input,
        "lstm_model.onnx",
        input_names=["input"],
        output_names=["output"],
        dynamic_axes={"input": {0: "batch_size"}, "output": {0: "batch_size"}},
        opset_version=12
    )
    print("[INFO] LSTM ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†")
    return model

def extract_high_accuracy_combinations(evaluation_df, threshold=2):
    high_matches = evaluation_df[evaluation_df["æœ¬æ•°å­—ä¸€è‡´æ•°"] >= threshold]
    return high_matches

def transform_to_digit_labels(numbers_series):
    y1, y2, y3 = [], [], []
    for entry in numbers_series:
        digits = [int(d) for d in re.findall(r'\d', str(entry))[:3]]
        if len(digits) == 3:
            y1.append(digits[0])
            y2.append(digits[1])
            y3.append(digits[2])
    return y1, y2, y3

# --- ğŸ” çš„ä¸­äºˆæ¸¬æŠ½å‡ºï¼ˆTransfer Learning ç”¨ï¼‰ ---
def extract_matched_predictions(predictions, true_data, min_match=2):
    matched = []
    for pred in predictions:
        for true in true_data:
            if len(set(pred) & set(true)) >= min_match:
                matched.append(pred)
                break
    return matched

def reinforce_top_features(X, feature_names, target_scores, top_n=5):
    corrs = {
        feat: abs(np.corrcoef(X[:, i], target_scores)[0, 1])
        for i, feat in enumerate(feature_names)
    }
    top_feats = sorted(corrs.items(), key=lambda x: -x[1])[:top_n]
    reinforced_X = X.copy()
    for feat, _ in top_feats:
        idx = feature_names.index(feat)
        reinforced_X[:, idx] *= 1.5
    return reinforced_X

class MemoryEncoder(nn.Module):
    def __init__(self, vocab_size=10, embed_dim=64, num_layers=2, nhead=4):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)

    def forward(self, x):
        x = self.embedding(x)  # x: [seq_len, batch]
        return self.encoder(x)  # return shape: [seq_len, batch, embed_dim]

class GPT4Numbers(nn.Module):
    def __init__(self, vocab_size=10, embed_dim=64, num_heads=4, num_layers=3):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_encoding = PositionalEncoding(embed_dim)
        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)
        self.fc_out = nn.Linear(embed_dim, vocab_size)

    def forward(self, tgt, memory):
        tgt_embed = self.embedding(tgt)  # (seq_len, batch, embed_dim)
        tgt_embed = self.pos_encoding(tgt_embed)  # (seq_len, batch, embed_dim)
        output = self.decoder(tgt_embed, memory)  # (seq_len, batch, embed_dim)
        return self.fc_out(output)  # (seq_len, batch, vocab_size)

def build_memory_from_history(history_sequences, encoder, device):
    if not history_sequences:
        return torch.zeros((1, 1, encoder.embedding.embedding_dim), device=device)

    max_len = max(len(seq) for seq in history_sequences)
    padded = [seq + [0] * (max_len - len(seq)) for seq in history_sequences]
    tensor = torch.tensor(padded, dtype=torch.long).T.to(device)  # shape: [seq_len, batch]

    # âš ï¸ ä¿®æ­£ï¼šã“ã“ã§1ä»¶ã ã‘ã®å±¥æ­´ã«çµã‚‹
    tensor = tensor[:, :1]  # â† ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’1ã«åˆ¶é™

    with torch.no_grad():
        memory = encoder(tensor)  # shape: [seq_len, 1, embed_dim]
    return memory

def train_gpt3numbers_model_with_memory(
    save_path="gpt3numbers.pth",
    encoder_path="memory_encoder_3.pth",
    epochs=50
):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    encoder = MemoryEncoder().to(device)
    decoder = GPT4Numbers().to(device)
    optimizer = torch.optim.Adam(list(decoder.parameters()) + list(encoder.parameters()), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    try:
        df = pd.read_csv("numbers3.csv")
        df["æœ¬æ•°å­—"] = df["æœ¬æ•°å­—"].apply(parse_number_string)
        sequences = [row for row in df["æœ¬æ•°å­—"] if isinstance(row, list) and len(row) == 3]
    except Exception as e:
        print(f"[ERROR] å­¦ç¿’ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return decoder, encoder

    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆ1ã€œ2æ¡ â†’ æ¬¡ã®1æ¡ï¼‰
    data = []
    for seq in sequences:
        for i in range(1, 3):
            context = seq[:i]
            target = seq[i]
            history = sequences[:sequences.index(seq)]
            data.append((context, target, history[-10:]))

    if not data:
        print("[WARNING] GPT3Numbers å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        return decoder, encoder

    print(f"[INFO] GPT3Numbers å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(data)}")

    for epoch in range(epochs):
        random.shuffle(data)
        total_loss = 0
        for context, target, hist in data:
            tgt = torch.tensor(context, dtype=torch.long).unsqueeze(1).to(device)
            target_tensor = torch.tensor([target], dtype=torch.long).to(device)

            memory = build_memory_from_history(hist, encoder, device)

            output = decoder(tgt, memory)
            last_output = output[-1, 0].unsqueeze(0)

            loss = criterion(last_output, target_tensor)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        if (epoch + 1) % 10 == 0 or epoch == 0:
            print(f"[GPT3-MEM] Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(data):.4f}")

    torch.save(decoder.state_dict(), save_path)
    torch.save(encoder.state_dict(), encoder_path)
    print(f"[INFO] GPT3Numbers ä¿å­˜: {save_path}")
    print(f"[INFO] MemoryEncoder ä¿å­˜: {encoder_path}")
    return decoder, encoder

def gpt_generate_predictions_with_memory_3(decoder, encoder, history_sequences, num_samples=5):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    decoder.eval()
    encoder.eval()
    predictions = []

    memory = build_memory_from_history(history_sequences[-10:], encoder, device)

    for _ in range(num_samples):
        seq = [random.randint(0, 9)]
        for _ in range(2):  # â†’ åˆè¨ˆ3æ¡ã«ãªã‚‹ã‚ˆã†ã«2å›ã ã‘è¿½åŠ 
            tgt = torch.tensor(seq, dtype=torch.long).unsqueeze(1).to(device)
            with torch.no_grad():
                logits = decoder(tgt, memory)
                next_digit = int(torch.argmax(logits[-1]).item())
            seq.append(next_digit)

        if len(set(seq)) == 3:
            predictions.append((seq, 0.91))

    return predictions

def gpt_generate_predictions(model, num_samples=5, context_length=4):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    predictions = []
    for _ in range(num_samples):
        seq = [random.randint(0, 9)]
        for _ in range(context_length - 1):
            tgt = torch.tensor(seq, dtype=torch.long).unsqueeze(1).to(device)  # shape: (seq_len, batch=1)
            embed_dim = model.embedding.embedding_dim
            memory = torch.zeros((1, 1, embed_dim), dtype=torch.float32).to(device)
            with torch.no_grad():
                logits = model(tgt, memory)
                next_token = torch.argmax(logits[-1]).item()
                seq.append(next_token)
        if len(set(seq)) == 4:
            predictions.append((seq, 0.89))  # ä¿¡é ¼åº¦ã¯ä»®
    return predictions

def train_gpt3numbers_model(save_path="gpt3numbers.pth", epochs=50):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = GPT4Numbers().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    try:
        df = pd.read_csv("numbers3.csv")
        df["æœ¬æ•°å­—"] = df["æœ¬æ•°å­—"].apply(parse_number_string)
        sequences = [row for row in df["æœ¬æ•°å­—"] if isinstance(row, list) and len(row) == 3]
    except Exception as e:
        print(f"[ERROR] å­¦ç¿’ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return model

    data = []
    for seq in sequences:
        for i in range(len(seq) - 1):  # ä¾‹: [2,5,3] â†’ ([2],5), ([2,5],3)
            context = seq[:i + 1]
            target = seq[i + 1]
            if len(context) >= 1:
                data.append((context, target))

    if not data:
        print("[WARNING] GPT3Numbers å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        return model

    print(f"[INFO] GPT3Numbers å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(data)}")

    for epoch in range(epochs):
        random.shuffle(data)
        total_loss = 0
        for context, target in data:
            tgt = torch.tensor(context, dtype=torch.long).unsqueeze(1).to(device)
            memory = torch.zeros((1, 1, model.embedding.embedding_dim), dtype=torch.float32).to(device)
            target_tensor = torch.tensor([target], dtype=torch.long).to(device)

            output = model(tgt, memory)[-1].unsqueeze(0)
            loss = criterion(output, target_tensor)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        if (epoch + 1) % 10 == 0 or epoch == 0:
            avg_loss = total_loss / len(data)
            print(f"[GPT3] Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}")

    torch.save(model.state_dict(), save_path)
    print(f"[INFO] GPT3Numbers ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")
    return model

class LotoPredictor:
    def __init__(self, input_size, hidden_size):
        print("[INFO] ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–")
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.lstm_model = None
        self.regression_models = [None] * 3
        self.scaler = None
        self.feature_names = None
        self.meta_model = None

    def train_model(self, data):
        print("[INFO] Numbers3å­¦ç¿’é–‹å§‹")
        true_numbers = data['æœ¬æ•°å­—'].apply(lambda x: parse_number_string(x)).tolist()

        # é«˜ç­‰ç´šä¸€è‡´ã®ã¿è‡ªå·±äºˆæ¸¬ã‹ã‚‰å†æŠ•å…¥
        self_data = load_self_predictions(file_path="self_predictions.csv", min_match_threshold=2, true_data=true_numbers)
        if self_data:
            high_grade_predictions = []
            seen = set()
            for pred_tuple, count in self_data:
                pred = list(pred_tuple)
                if len(pred) != 3 or tuple(pred) in seen:
                    continue
                for true in true_numbers:
                    if classify_numbers3_prize(pred, true) in ["ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ", "ãƒœãƒƒã‚¯ã‚¹"]:
                        high_grade_predictions.append((pred, count))
                        seen.add(tuple(pred))
                        break

            if high_grade_predictions:
                synthetic_rows = pd.DataFrame({
                    'æŠ½ã›ã‚“æ—¥': pd.Timestamp.now(),
                    'æœ¬æ•°å­—': [row[0] for row in high_grade_predictions for _ in range(row[1])]
                })
                data = pd.concat([data, synthetic_rows], ignore_index=True)
                print(f"[INFO] è‡ªå·±é€²åŒ–ãƒ‡ãƒ¼ã‚¿è¿½åŠ : {len(synthetic_rows)}ä»¶")

        X, _, self.scaler = preprocess_data(data)
        if X is None:
            return

        processed_data = create_advanced_features(data)
        y1, y2, y3 = transform_to_digit_labels(processed_data['æœ¬æ•°å­—'])[:3]
        self.feature_names = [str(i) for i in range(X.shape[1])]
        X = reinforce_top_features(X, self.feature_names, y1)
        X_df = pd.DataFrame(X, columns=self.feature_names)

        for i, y in enumerate([y1, y2, y3]):
            df_train = X_df.copy()
            df_train['target'] = y
            predictor = TabularPredictor(label='target', path=f'autogluon_n3_pos{i}').fit(df_train, time_limit=300)
            self.regression_models[i] = predictor
            print(f"[AutoGluon] ãƒ¢ãƒ‡ãƒ« {i+1}/3 å®Œäº†")

        # LSTMè¨“ç·´
        input_size = X.shape[1]
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = LotoLSTM(input_size, 128).to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = torch.nn.CrossEntropyLoss()

        X_tensor = torch.tensor(X.reshape(-1, 1, input_size), dtype=torch.float32).to(device)
        y_tensors = [torch.tensor(y, dtype=torch.long).to(device) for y in [y1, y2, y3]]
        dataset = TensorDataset(X_tensor, *y_tensors)
        loader = DataLoader(dataset, batch_size=32, shuffle=True)

        model.train()
        for epoch in range(50):
            total_loss = 0
            for batch in loader:
                inputs = batch[0]
                targets = batch[1:]
                optimizer.zero_grad()
                outputs = model(inputs)[:3]  # 3æ¡ã«é™å®š
                losses = [criterion(out, target) for out, target in zip(outputs, targets)]
                loss = sum(losses)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            print(f"[LSTM] Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}")

        self.lstm_model = model
        self.meta_model = train_meta_model_maml("evaluation_result.csv", data)

    def predict(self, latest_data, num_candidates=50):
        print("[INFO] Numbers3äºˆæ¸¬é–‹å§‹")
        X, _, _ = preprocess_data(latest_data)
        if X is None:
            return None, None

        X_df = pd.DataFrame(X, columns=self.feature_names)
        pred_digits = []
        for i in range(3):  # 3æ¡
            pred = self.regression_models[i].predict(X_df)
            pred_digits.append(pred)
        auto_preds = np.array(pred_digits).T

        input_size = X.shape[1]
        X_tensor = torch.tensor(X.reshape(-1, 1, input_size), dtype=torch.float32)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.lstm_model.to(device)
        self.lstm_model.eval()
        with torch.no_grad():
            outputs = self.lstm_model(X_tensor.to(device))[:3]
            lstm_preds = [torch.argmax(out, dim=1).cpu().numpy() for out in outputs]
        lstm_preds = np.array(lstm_preds).T

        all_predictions = []
        for i in range(min(len(auto_preds), len(lstm_preds))):
            merged = (0.7 * auto_preds[i] + 0.3 * lstm_preds[i]).round().astype(int)
            numbers = list(map(int, merged))

            if len(set(numbers)) < 3:
                continue
            if score_real_structure_similarity(numbers) < 0.3:
                continue

            base_conf = 1.0
            corrected_conf = base_conf
            if self.meta_model:
                try:
                    feat_vec = X_df.iloc[i].values.reshape(1, -1)
                    predicted_match = self.meta_model.predict(feat_vec)[0]
                    corrected_conf = max(0.0, min(predicted_match / 3.0, 1.0))
                    final_conf = 0.5 * base_conf + 0.5 * corrected_conf
                except:
                    final_conf = base_conf
            else:
                final_conf = base_conf

            all_predictions.append((numbers, final_conf))

        return all_predictions[:num_candidates], [c for _, c in all_predictions[:num_candidates]]

def classify_numbers3_prize(pred, actual):
    pred = list(map(int, pred))
    actual = list(map(int, actual))

    if pred == actual:
        return "ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ"
    elif sorted(pred) == sorted(actual):
        return "ãƒœãƒƒã‚¯ã‚¹"
    elif pred[1:] == actual[1:]:
        return "ãƒŸãƒ‹"
    else:
        return "ã¯ãšã‚Œ"

def simulate_grade_distribution(simulated, historical_numbers):
    counter = Counter()
    for sim in simulated:
        for actual in historical_numbers:
            prize = classify_numbers3_prize(sim, actual)
            counter[prize] += 1
            break  # 1å›ã®ä¸€è‡´ã§è‰¯ã„

    total = sum(counter.values())
    return {k: v / total for k, v in counter.items()}

# äºˆæ¸¬çµæœã®è©•ä¾¡
def evaluate_predictions(predictions, actual_numbers):
    results = []
    for pred in predictions:
        match_type = classify_numbers3_prize(pred[0], actual_numbers)
        if match_type == "ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ":
            reward = 900000
        elif match_type == "ãƒœãƒƒã‚¯ã‚¹":
            reward = 37500
        else:
            reward = 0
            
        results.append({
            'äºˆæ¸¬': pred[0],
            'ä¸€è‡´æ•°': len(set(pred[0]) & set(actual_numbers)),
            'ç­‰ç´š': match_type,
            'ä¿¡é ¼åº¦': pred[1],
            'æœŸå¾…åç›Š': reward
        })
    return results

official_url = "https://www.takarakuji-official.jp/ec/numbers3/"

async def fetch_drawing_dates():
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(official_url, timeout=10) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    drawing_dates = []
                    # ä¿®æ­£ã•ã‚ŒãŸæŠ½ã›ã‚“æ—¥ã‚’å–å¾—
                    date_elements = soup.select("p.m_ttlSet_sttl")
                    for p in date_elements:
                        text = p.get_text(strip=True)
                        if "æŠ½ã›ã‚“æ—¥ï¼š" in text:
                            date_part = text.replace("æŠ½ã›ã‚“æ—¥ï¼š", "").strip()
                            formatted_date = date_part.replace("/", "-")
                            drawing_dates.append(formatted_date)

                    return drawing_dates
                else:
                    print(f"HTTPã‚¨ãƒ©ãƒ¼ {response.status}: {official_url}")
        except Exception as e:
            print(f"æŠ½ã›ã‚“æ—¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    return []

# æœ€æ–°ã®æŠ½ã›ã‚“æ—¥ã‚’1ä»¶ã ã‘å–å¾—ã™ã‚‹é–¢æ•°ï¼ˆå¿…è¦ãªã‚‰ï¼‰
async def get_latest_drawing_date():
    dates = await fetch_drawing_dates()
    return dates[0] if dates else None


def parse_number_string(number_str):
    """
    äºˆæ¸¬ç•ªå·ã‚„å½“é¸ç•ªå·ã®æ–‡å­—åˆ—ã‚’ãƒªã‚¹ãƒˆåŒ–ã™ã‚‹é–¢æ•°
    - ã‚¹ãƒšãƒ¼ã‚¹ / ã‚«ãƒ³ãƒ / ã‚¿ãƒ– åŒºåˆ‡ã‚Šã«å¯¾å¿œ
    - "07 15 20 28 29 34 36" â†’ [7, 15, 20, 28, 29, 34, 36]
    - "[7, 15, 20, 28, 29, 34, 36]" â†’ [7, 15, 20, 28, 29, 34, 36]
    """
    if pd.isna(number_str):
        return []  # NaN ã®å ´åˆã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
    
    # ä¸è¦ãªè¨˜å·ã‚’å‰Šé™¤ï¼ˆãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆï¼‰
    number_str = number_str.strip("[]").replace("'", "").replace('"', '')

    # ã‚¹ãƒšãƒ¼ã‚¹ãƒ»ã‚«ãƒ³ãƒãƒ»ã‚¿ãƒ–ã§åˆ†å‰²ã—ã€æ•´æ•°å¤‰æ›
    numbers = re.split(r'[\s,]+', number_str)

    # æ•°å­—ã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦æ•´æ•°å¤‰æ›
    return [int(n) for n in numbers if n.isdigit()]

def calculate_precision_recall_f1(evaluation_df):
    y_true = []
    y_pred = []

    for _, row in evaluation_df.iterrows():
        actual = set(row["å½“é¸æœ¬æ•°å­—"])
        predicted = set(row["äºˆæ¸¬ç•ªå·"])
        for n in range(0, 10):
            y_true.append(1 if n in actual else 0)
            y_pred.append(1 if n in predicted else 0)

    try:
        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
    except Exception as e:
        print("[ERROR] è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—ã«å¤±æ•—:", e)
        precision, recall, f1 = 0, 0, 0

    print("\n=== è©•ä¾¡æŒ‡æ¨™ ===")
    print(f"Precision: {precision:.3f}")
    print(f"Recall:    {recall:.3f}")
    print(f"F1 Score:  {f1:.3f}")

# äºˆæ¸¬çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹é–¢æ•°
def save_predictions_to_csv(predictions, drawing_date, filename="Numbers3_predictions.csv"):
    drawing_date = pd.to_datetime(drawing_date).strftime("%Y-%m-%d")
    row = {"æŠ½ã›ã‚“æ—¥": drawing_date}

    for i, (numbers, confidence) in enumerate(predictions[:5], 1):
        row[f"äºˆæ¸¬{i}"] = ', '.join(map(str, numbers))
        row[f"ä¿¡é ¼åº¦{i}"] = round(confidence, 3)
        
    df = pd.DataFrame([row])

    if os.path.exists(filename):
        try:
            existing_df = pd.read_csv(filename, encoding='utf-8-sig')
            if "æŠ½ã›ã‚“æ—¥" not in existing_df.columns:
                print(f"è­¦å‘Š: CSVã«'æŠ½ã›ã‚“æ—¥'åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
                default_columns = ["æŠ½ã›ã‚“æ—¥"] + [f"äºˆæ¸¬{i}" for i in range(1, 6)] + [f"ä¿¡é ¼åº¦{i}" for i in range(1, 6)]
                existing_df = pd.DataFrame(columns=default_columns)
            existing_df = existing_df[existing_df["æŠ½ã›ã‚“æ—¥"] != drawing_date]
            df = pd.concat([existing_df, df], ignore_index=True)
        except Exception as e:
            print(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
            df = pd.DataFrame([row])

    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"äºˆæ¸¬çµæœã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

def is_running_with_streamlit():
    try:
        from streamlit.runtime.scriptrunner import get_script_run_ctx
        return get_script_run_ctx() is not None
    except ImportError:
        return False

def ppo_multiagent_predict(historical_data, num_predictions=5):
    """
    ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆå¯„ã›ãƒ»ãƒœãƒƒã‚¯ã‚¹å¯„ã›ãƒ»ãƒ©ãƒ³ãƒ€ãƒ æœ€å¤§åŒ–ã®3æ–¹é‡ã§PPOã‚’å®Ÿè¡Œã—ã€
    é«˜ä¿¡é ¼åº¦ã®å‡ºåŠ›ã‚’é¸æŠœã—ã¦è¿”ã™ã€‚
    """
    from random import randint
    agents = ["straight", "box", "diverse"]
    results = []

    for strategy in agents:
        for _ in range(num_predictions):
            if strategy == "straight":
                # ç›´è¿‘ã®å‡ºç¾æ•°å­—ã‹ã‚‰ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆæ§‹æˆã‚’å„ªå…ˆ
                last = parse_number_string(historical_data.iloc[-1]["æœ¬æ•°å­—"])
                new = list((n + randint(0, 2)) % 10 for n in last)
                confidence = 0.91
            elif strategy == "box":
                # ãƒœãƒƒã‚¯ã‚¹å¯„ã›ï¼šé »å‡ºæ•°å­—ã‚’é›†ã‚ã¦é †ä¸åŒæ§‹æˆ
                all_nums = [n for row in historical_data["æœ¬æ•°å­—"] for n in parse_number_string(row)]
                freq = Counter(all_nums).most_common(4)
                new = sorted([f[0] for f in freq])
                confidence = 0.92
            else:
                # å¤šæ§˜æ€§æœ€å¤§åŒ–ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã«ã°ã‚‰ã‘ãŸæ§‹æˆï¼‰
                new = sorted(randint(0, 9) for _ in range(3))
                confidence = 0.905

            results.append((new, confidence))

    return results

def train_diffusion_model(df, model_path="diffusion_model.pth", epochs=100, device="cpu"):
    class DiffusionMLP(nn.Module):
        def __init__(self, input_dim=3, hidden_dim=64):
            super().__init__()
            self.net = nn.Sequential(
                nn.Linear(input_dim + 1, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, input_dim)
            )

        def forward(self, x, t):
            t_embed = t.float().view(-1, 1) / 1000.0
            x_input = torch.cat([x, t_embed], dim=1)
            return self.net(x_input)

    def prepare_training_data(df):
        data = []
        for row in df["æœ¬æ•°å­—"]:
            nums = parse_number_string(row)
            if len(nums) == 3:
                data.append(nums)
        return np.array(data)

    model = DiffusionMLP().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    T = 1000
    def noise_schedule(t): return 1 - t / T

    X = prepare_training_data(df)
    if len(X) == 0:
        print("[ERROR] Diffusionå­¦ç¿’ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        return

    data = torch.tensor(X, dtype=torch.float32).to(device)

    for epoch in range(epochs):
        last_loss = None
        for i in range(len(data)):
            x0 = data[i].unsqueeze(0)
            t = torch.randint(1, T, (1,), device=device)
            alpha = noise_schedule(t)
            noise = torch.randn_like(x0)
            xt = torch.sqrt(alpha) * x0 + torch.sqrt(1 - alpha) * noise

            pred_noise = model(xt, t)
            loss = F.mse_loss(pred_noise, noise)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            last_loss = loss

        if last_loss is not None and (epoch + 1) % 10 == 0:
            print(f"[Epoch {epoch+1}] Loss: {last_loss.item():.4f}")

    torch.save(model.state_dict(), model_path)
    print(f"[INFO] Diffusion ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {model_path}")

def diffusion_generate_predictions(df, num_predictions=5, model_path="diffusion_model.pth"):
    class DiffusionMLP(nn.Module):
        def __init__(self, input_dim=3, hidden_dim=64):
            super().__init__()
            self.net = nn.Sequential(
                nn.Linear(input_dim + 1, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, input_dim)
            )

        def forward(self, x, t):
            t_embed = t.float().view(-1, 1) / 1000.0
            x_input = torch.cat([x, t_embed], dim=1)
            return self.net(x_input)

    model = DiffusionMLP()
    model.load_state_dict(torch.load(model_path, map_location=torch.device("cpu")))
    model.eval()

    predictions = []
    trials = 0
    max_trials = num_predictions * 10

    while len(predictions) < num_predictions and trials < max_trials:
        trials += 1
        x = torch.randn(1, 3)
        timesteps = list(range(1000))[::-1]
        for t in timesteps:
            t_tensor = torch.tensor([t]).float().view(-1, 1)
            noise_pred = model(x, t_tensor)
            x = x - noise_pred / 1000.0

        candidate = tuple(int(round(v)) for v in x.squeeze().tolist())

        if all(0 <= n <= 9 for n in candidate) and len(set(candidate)) == 3:
            predictions.append(candidate)

    return [(list(p), 0.91) for p in predictions]

def load_trained_model():
    print("[INFO] å¤–éƒ¨ãƒ¢ãƒ‡ãƒ«ã¯æœªå®šç¾©ã®ãŸã‚ã€Noneã‚’è¿”ã—ã¾ã™ã€‚")
    return None

class CycleAttentionTransformer(nn.Module):
    def __init__(self, input_dim, embed_dim=64, num_heads=4, num_layers=2):
        super(CycleAttentionTransformer, self).__init__()
        self.embedding = nn.Linear(input_dim, embed_dim)
        self.pos_encoder = PositionalEncoding(embed_dim)
        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.ff = nn.Sequential(
            nn.Linear(embed_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 4)
        )

    def forward(self, x):
        x = self.embedding(x)                      # (batch, seq_len, embed_dim)
        x = self.pos_encoder(x)                    # (batch, seq_len, embed_dim)
        x = x.permute(1, 0, 2)                     # (seq_len, batch, embed_dim)
        x = self.transformer_encoder(x)            # (seq_len, batch, embed_dim)
        x = x.permute(1, 0, 2)                     # (batch, seq_len, embed_dim)
        x = x.mean(dim=1)                          # Global average pooling
        x = self.ff(x)                             # (batch, 4)
        return x

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0)]
        return x
    
def train_transformer_with_cycle_attention(df, model_path="transformer_model.pth", epochs=50):
    print("[INFO] Transformerãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")

    class CycleAttentionTransformer(nn.Module):
        def __init__(self, input_dim, embed_dim=64, num_heads=4, num_layers=2):
            super().__init__()
            self.embedding = nn.Linear(input_dim, embed_dim)
            self.pos_encoder = PositionalEncoding(embed_dim)
            encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)
            self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
            self.ff = nn.Sequential(
                nn.Linear(embed_dim, 128),
                nn.ReLU(),
                nn.Linear(128, 3)  # â† â­ 3æ¡ã«ä¿®æ­£
            )

        def forward(self, x):
            x = self.embedding(x)
            x = self.pos_encoder(x)
            x = x.permute(1, 0, 2)
            x = self.transformer_encoder(x)
            x = x.permute(1, 0, 2)
            x = x.mean(dim=1)
            return self.ff(x)

    def prepare_input(df):
        recent = df.tail(10)
        nums = [parse_number_string(n) for n in recent["æœ¬æ•°å­—"]]
        flat = [n for row in nums for n in row]
        flat = (flat + [0] * 40)[:40]
        return torch.tensor([flat], dtype=torch.float32)

    model = CycleAttentionTransformer(input_dim=40)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    loss_fn = nn.MSELoss()

    for epoch in range(epochs):
        x = prepare_input(df)
        y = torch.tensor([[random.randint(0, 9) for _ in range(3)]], dtype=torch.float32)
        pred = model(x)
        loss = loss_fn(pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 10 == 0:
            print(f"[Transformer] Epoch {epoch+1}, Loss: {loss.item():.4f}")

    torch.save(model.state_dict(), model_path)
    print(f"[INFO] Transformerãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {model_path}")

def transformer_generate_predictions(df, model_path="transformer_model.pth"):
    class CycleAttentionTransformer(nn.Module):
        def __init__(self, input_dim=40, embed_dim=64, num_heads=4, num_layers=2):
            super().__init__()
            self.embedding = nn.Linear(input_dim, embed_dim)
            self.pos_encoder = PositionalEncoding(embed_dim)
            encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)
            self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
            self.ff = nn.Sequential(
                nn.Linear(embed_dim, 128),
                nn.ReLU(),
                nn.Linear(128, 3)  # â† â­ 3æ¡å‡ºåŠ›
            )

        def forward(self, x):
            x = self.embedding(x)
            x = self.pos_encoder(x)
            x = x.permute(1, 0, 2)
            x = self.transformer_encoder(x)
            x = x.permute(1, 0, 2)
            return self.ff(x.mean(dim=1))

    def prepare_input(df):
        recent = df.tail(10)
        nums = [parse_number_string(n) for n in recent["æœ¬æ•°å­—"]]
        flat = [n for row in nums for n in row]
        flat = (flat + [0] * 40)[:40]
        return torch.tensor([flat], dtype=torch.float32)

    input_tensor = prepare_input(df)

    model = CycleAttentionTransformer()
    if not os.path.exists(model_path):
        train_transformer_with_cycle_attention(df, model_path=model_path)
    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
    model.eval()

    with torch.no_grad():
        output = model(input_tensor)
        prediction = [max(0, min(9, int(round(p.item())))) for p in output.squeeze()]
        print(f"[Transformer] äºˆæ¸¬çµæœ: {prediction}")
        return [(prediction, 0.95)]

def evaluate_and_summarize_predictions(
    pred_file="numbers3_predictions.csv",
    actual_file="numbers3.csv",
    output_csv="evaluation_result.csv",
    output_txt="evaluation_summary.txt"
):
    try:
        pred_df = pd.read_csv(pred_file)
        actual_df = pd.read_csv(actual_file)
        actual_df['æŠ½ã›ã‚“æ—¥'] = pd.to_datetime(actual_df['æŠ½ã›ã‚“æ—¥'], errors='coerce').dt.date
        pred_df['æŠ½ã›ã‚“æ—¥'] = pd.to_datetime(pred_df['æŠ½ã›ã‚“æ—¥'], errors='coerce').dt.date
    except Exception as e:
        print(f"[ERROR] ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return

    evaluation_results = []
    grade_counter = Counter()
    match_counter = Counter()
    all_hits = []

    grade_list = ["ã¯ãšã‚Œ", "ãƒŸãƒ‹", "ãƒœãƒƒã‚¯ã‚¹", "ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ"]

    results_by_prediction = {
        i: {grade: 0 for grade in grade_list} | {"details": []}
        for i in range(1, 6)
    }

    for _, row in pred_df.iterrows():
        draw_date = row["æŠ½ã›ã‚“æ—¥"]
        actual_row = actual_df[actual_df["æŠ½ã›ã‚“æ—¥"] == draw_date]
        if actual_row.empty:
            continue
        actual_numbers = parse_number_string(actual_row.iloc[0]["æœ¬æ•°å­—"])

        for i in range(1, 6):
            pred_key = f"äºˆæ¸¬{i}"
            conf_key = f"ä¿¡é ¼åº¦{i}"
            if pred_key in row and pd.notna(row[pred_key]):
                predicted = parse_number_string(str(row[pred_key]))
                confidence = row[conf_key] if conf_key in row and pd.notna(row[conf_key]) else 1.0
                grade = classify_numbers3_prize(predicted, actual_numbers)
                match_count = len(set(predicted) & set(actual_numbers))

                if 0.91 <= confidence <= 0.93:
                    source = "PPO/Diffusion"
                elif confidence > 0.93:
                    source = "BaseModel"
                else:
                    source = "Unknown"

                evaluation_results.append({
                    "æŠ½ã›ã‚“æ—¥": draw_date.strftime("%Y-%m-%d"),
                    "äºˆæ¸¬ç•ªå·": predicted,
                    "å½“é¸æœ¬æ•°å­—": actual_numbers,
                    "ä¸€è‡´æ•°": match_count,
                    "ç­‰ç´š": grade,
                    "ä¿¡é ¼åº¦": confidence,
                    "å‡ºåŠ›å…ƒ": source,
                    "äºˆæ¸¬ç•ªå·ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹": f"äºˆæ¸¬{i}"
                })

                grade_counter[grade] += 1
                match_counter[match_count] += 1
                results_by_prediction[i][grade] += 1

                if grade != "ã¯ãšã‚Œ":
                    detail = f'{draw_date},"{predicted}","{actual_numbers}",{grade}'
                    results_by_prediction[i]["details"].append(detail)
                    all_hits.append(detail)

    # === CSVä¿å­˜ ===
    eval_df = pd.DataFrame(evaluation_results)
    eval_df.to_csv(output_csv, index=False, encoding="utf-8-sig")
    print(f"[INFO] æ¯”è¼ƒçµæœã‚’ {output_csv} ã«ä¿å­˜ã—ã¾ã—ãŸ")

    # === TXTå‡ºåŠ› ===
    lines = []
    lines.append("== ç­‰ç´šåˆ¥å…¨ä½“é›†è¨ˆ ==")
    for k in grade_list:
        lines.append(f"{k}: {grade_counter[k]} ä»¶")

    total = sum(grade_counter.values())
    matched = grade_counter["ãƒŸãƒ‹"] + grade_counter["ãƒœãƒƒã‚¯ã‚¹"] + grade_counter["ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ"]
    rate = (matched / total * 100) if total > 0 else 0
    lines.append("\n== ç­‰ç´šçš„ä¸­ç‡ãƒã‚§ãƒƒã‚¯ ==")
    lines.append(f"ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆãƒ»ãƒœãƒƒã‚¯ã‚¹ãƒ»ãƒŸãƒ‹ã®åˆè¨ˆ: {matched} ä»¶")
    lines.append(f"çš„ä¸­ç‡ï¼ˆç­‰ç´šãƒ™ãƒ¼ã‚¹ï¼‰: {rate:.2f}%")
    lines.append("âœ“ çš„ä¸­ç‡ã¯ç›®æ¨™ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚" if rate >= 10 else "âœ˜ çš„ä¸­ç‡ã¯ç›®æ¨™ã«é”ã—ã¦ã„ã¾ã›ã‚“ã€‚")

    # === å„äºˆæ¸¬ç•ªå·ã®ç­‰ç´šãƒ»è³é‡‘ãƒ»åˆ©ç›Šã®é›†è¨ˆ ===
    lines.append("\n== å„äºˆæ¸¬ã”ã¨ã®è³é‡‘ãƒ»ã‚³ã‚¹ãƒˆãƒ»åˆ©ç›Š ==")
    mini_prize = 9000
    box_prize = 15000
    straight_prize = 114000
    cost_per_draw = 600

    for i in range(1, 6):
        lines.append(f"\n== ç­‰ç´šåˆ¥äºˆæƒ³{i}é›†è¨ˆ ==")
        mini = results_by_prediction[i]["ãƒŸãƒ‹"]
        box = results_by_prediction[i]["ãƒœãƒƒã‚¯ã‚¹"]
        straight = results_by_prediction[i]["ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ"]

        for k in grade_list:
            lines.append(f"{k}: {results_by_prediction[i][k]} ä»¶")

        hit_count = mini + box + straight
        total_preds = sum(results_by_prediction[i][k] for k in grade_list)
        acc = (hit_count / total_preds * 100) if total_preds > 0 else 0
        lines.append("\n== ç­‰ç´šçš„ä¸­ç‡ãƒã‚§ãƒƒã‚¯ ==")
        lines.append(f"ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆãƒ»ãƒœãƒƒã‚¯ã‚¹ãƒ»ãƒŸãƒ‹ã®åˆè¨ˆ: {hit_count} ä»¶")
        lines.append(f"çš„ä¸­ç‡ï¼ˆç­‰ç´šãƒ™ãƒ¼ã‚¹ï¼‰: {acc:.2f}%")

        for detail in results_by_prediction[i]["details"]:
            lines.append(detail)

        # è³é‡‘ã¨åˆ©ç›Šã®è¨ˆç®—
        mini_total = mini * mini_prize
        box_total = box * box_prize
        straight_total = straight * straight_prize
        total_reward = mini_total + box_total + straight_total
        cost = total_preds * cost_per_draw
        profit = total_reward - cost

        lines.append(f"\n== äºˆæ¸¬{i}ã®è³é‡‘ãƒ»æç›Š ==")
        lines.append(f"ãƒŸãƒ‹: {mini} ä»¶ Ã— Â¥{mini_prize:,} = Â¥{mini_total:,}")
        lines.append(f"ãƒœãƒƒã‚¯ã‚¹: {box} ä»¶ Ã— Â¥{box_prize:,} = Â¥{box_total:,}")
        lines.append(f"ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ: {straight} ä»¶ Ã— Â¥{straight_prize:,} = Â¥{straight_total:,}")
        lines.append(f"å½“é¸åˆè¨ˆé‡‘é¡: Â¥{total_reward:,}")
        lines.append(f"ã‚³ã‚¹ãƒˆï¼ˆ{total_preds} Ã— Â¥{cost_per_draw:,}ï¼‰: Â¥{cost:,}")
        lines.append(f"æç›Š: {'+' if profit >= 0 else '-'}Â¥{abs(profit):,}")

    # === å…¨ä½“ã®è³é‡‘ãƒ»åˆ©ç›Šã®é›†è¨ˆ ===
    lines.append("\n== è³é‡‘ãƒ»ã‚³ã‚¹ãƒˆãƒ»åˆ©ç›Šï¼ˆå…¨ä½“ï¼‰ ==")
    mini_total = grade_counter["ãƒŸãƒ‹"] * mini_prize
    box_total = grade_counter["ãƒœãƒƒã‚¯ã‚¹"] * box_prize
    straight_total = grade_counter["ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ"] * straight_prize
    all_reward = mini_total + box_total + straight_total
    total_cost = total * cost_per_draw
    profit = all_reward - total_cost

    lines.append(f"ãƒŸãƒ‹: {grade_counter['ãƒŸãƒ‹']} ä»¶ Ã— Â¥{mini_prize:,} = Â¥{mini_total:,}")
    lines.append(f"ãƒœãƒƒã‚¯ã‚¹: {grade_counter['ãƒœãƒƒã‚¯ã‚¹']} ä»¶ Ã— Â¥{box_prize:,} = Â¥{box_total:,}")
    lines.append(f"ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ: {grade_counter['ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ']} ä»¶ Ã— Â¥{straight_prize:,} = Â¥{straight_total:,}")
    lines.append(f"å½“é¸åˆè¨ˆé‡‘é¡(ç†è«–å€¤): Â¥{all_reward:,}")
    lines.append(f"ç·ã‚³ã‚¹ãƒˆï¼ˆå…¨ä½“ä»¶æ•° {total} Ã— Â¥{cost_per_draw:,}ï¼‰: Â¥{total_cost:,}")
    lines.append(f"æœ€çµ‚æç›Š: {'+' if profit >= 0 else '-'}Â¥{abs(profit):,}")

    with open(output_txt, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"[INFO] é›†è¨ˆçµæœã‚’ {output_txt} ã«å‡ºåŠ›ã—ã¾ã—ãŸï¼ˆ{len(all_hits)} ä»¶ã®çš„ä¸­ï¼‰")

def main_with_improved_predictions():
    try:
        df = pd.read_csv("numbers3.csv")
        df["æŠ½ã›ã‚“æ—¥"] = pd.to_datetime(df["æŠ½ã›ã‚“æ—¥"], errors='coerce')
        df = df.sort_values("æŠ½ã›ã‚“æ—¥").reset_index(drop=True)
    except Exception as e:
        print(f"[ERROR] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return

    historical_data = df.copy()
    latest_date = historical_data["æŠ½ã›ã‚“æ—¥"].max()
    next_draw_date = latest_date + pd.Timedelta(days=1)

    all_groups = {
        "PPO": ppo_multiagent_predict(historical_data),
        "Diffusion": diffusion_generate_predictions(historical_data, 5),
        "Transformer": transformer_generate_predictions(historical_data),
    }

    # GPT (with memory)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    gpt_model_path = "gpt4numbers.pth"
    encoder_path = "memory_encoder.pth"
    if not os.path.exists(gpt_model_path) or not os.path.exists(encoder_path):
        decoder, encoder = train_gpt3numbers_model_with_memory(save_path=gpt_model_path, encoder_path=encoder_path)
    else:
        decoder = GPT4Numbers().to(device)
        encoder = MemoryEncoder().to(device)
        decoder.load_state_dict(torch.load(gpt_model_path, map_location=device))
        encoder.load_state_dict(torch.load(encoder_path, map_location=device))

    sequences = historical_data["æœ¬æ•°å­—"].apply(parse_number_string).tolist()
    all_groups["GPT"] = gpt_generate_predictions_with_memory_3(decoder, encoder, sequences, num_samples=5)

    # ã™ã¹ã¦ã®å€™è£œã‚’çµ±åˆ
    all_predictions = []
    for preds in all_groups.values():
        all_predictions.extend(preds)

    # çµ±åˆäºˆæ¸¬ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    verified = verify_predictions(all_predictions, historical_data)

    # ä¿å­˜å†…å®¹ã‚’æ§‹ç¯‰
    result = {"æŠ½ã›ã‚“æ—¥": next_draw_date.strftime("%Y-%m-%d")}
    for i, (numbers, conf) in enumerate(verified[:5]):
        result[f"äºˆæ¸¬{i+1}"] = ",".join(map(str, numbers))
        result[f"ä¿¡é ¼åº¦{i+1}"] = round(conf, 4)
        if 0.90 <= conf <= 0.93:
            result[f"å‡ºåŠ›å…ƒ{i+1}"] = "PPO/Diffusion/GPT"
        elif conf > 0.93:
            result[f"å‡ºåŠ›å…ƒ{i+1}"] = "Transformer"
        else:
            result[f"å‡ºåŠ›å…ƒ{i+1}"] = "Unknown"

    pred_path = "numbers3_predictions.csv"
    if os.path.exists(pred_path):
        pred_df = pd.read_csv(pred_path)
        pred_df = pred_df[pred_df["æŠ½ã›ã‚“æ—¥"] != next_draw_date.strftime("%Y-%m-%d")]
        pred_df = pd.concat([pred_df, pd.DataFrame([result])], ignore_index=True)
    else:
        pred_df = pd.DataFrame([result])

    pred_df.to_csv(pred_path, index=False, encoding='utf-8-sig')
    print(f"[INFO] æœ€æ–°äºˆæ¸¬ï¼ˆ{next_draw_date.strftime('%Y-%m-%d')}ï¼‰ã‚’ {pred_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")

    # âœ… çµ±åˆã•ã‚ŒãŸè©•ä¾¡ï¼‹é›†è¨ˆé–¢æ•°ã§å‡ºåŠ›
    try:
        evaluate_and_summarize_predictions(
            pred_file=pred_path,
            actual_file="numbers3.csv",
            output_csv="evaluation_result.csv",
            output_txt="evaluation_summary.txt"
        )
    except Exception as e:
        print(f"[WARNING] è©•ä¾¡å‡¦ç†ã«å¤±æ•—: {e}")

def calculate_pattern_score(numbers):
    score = 0
    if 10 <= sum(numbers) <= 20:  # åˆè¨ˆãŒã‚ã‚‹ç¨‹åº¦é«˜ã„
        score += 1
    if len(set(n % 2 for n in numbers)) > 1:  # å¶å¥‡ãŒæ··åœ¨
        score += 1
    if len(set(numbers)) == 3:  # é‡è¤‡ãªã—
        score += 1
    return score

def plot_prediction_analysis(predictions, historical_data):
    plt.figure(figsize=(15, 10))
    
    # äºˆæ¸¬ç•ªå·ã®åˆ†å¸ƒ
    plt.subplot(2, 2, 1)
    all_predicted_numbers = [num for pred in predictions for num in pred[0]]
    plt.hist(all_predicted_numbers, bins=9, range=(0, 9), alpha=0.7)
    plt.title('äºˆæ¸¬ç•ªå·ã®åˆ†å¸ƒ')
    plt.xlabel('æ•°å­—')
    plt.ylabel('é »åº¦')
    
    # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
    plt.subplot(2, 2, 2)
    confidence_scores = [pred[1] for pred in predictions]
    plt.hist(confidence_scores, bins=20, alpha=0.7)
    plt.title('ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ')
    plt.xlabel('ä¿¡é ¼åº¦')
    plt.ylabel('é »åº¦')
    
    # éå»ã®å½“é¸ç•ªå·ã¨ã®æ¯”è¼ƒ
    plt.subplot(2, 2, 3)
    historical_numbers = [num for numbers in historical_data['æœ¬æ•°å­—'] for num in numbers]
    plt.hist(historical_numbers, bins=9, range=(0, 9), alpha=0.5, label='éå»ã®å½“é¸')
    plt.hist(all_predicted_numbers, bins=9, range=(0, 9), alpha=0.5, label='äºˆæ¸¬')
    plt.title('äºˆæ¸¬ vs éå»ã®å½“é¸')
    plt.xlabel('æ•°å­—')
    plt.ylabel('é »åº¦')
    plt.legend()
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    plt.subplot(2, 2, 4)
    pattern_scores = [calculate_pattern_score(pred[0]) for pred in predictions]
    plt.scatter(range(len(pattern_scores)), pattern_scores, alpha=0.5)
    plt.title('äºˆæ¸¬ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¹ã‚³ã‚¢')
    plt.xlabel('äºˆæ¸¬ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹')
    plt.ylabel('ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¹ã‚³ã‚¢')
    
    plt.tight_layout()
    plt.savefig('prediction_analysis.png')
    plt.close()

def generate_evolution_graph(log_file="evolution_log.txt", output_file="evolution_graph.png"):
    """
    evolution_log.txtã‚’èª­ã¿è¾¼ã‚“ã§é€²åŒ–ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆãƒ»ä¿å­˜ã™ã‚‹
    """
    if not os.path.exists(log_file):
        print(f"[WARNING] é€²åŒ–ãƒ­ã‚° {log_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    dates = []
    counts = []

    with open(log_file, "r", encoding="utf-8") as f:
        for line in f:
            try:
                parts = line.strip().split(":")
                date_part = parts[0].strip()
                count_part = parts[2].strip()

                date = pd.to_datetime(date_part)
                count = int(count_part.split()[0])

                dates.append(date)
                counts.append(count)
            except Exception as e:
                print(f"[WARNING] ãƒ­ã‚°ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {e}")
                continue

    if not dates:
        print("[WARNING] é€²åŒ–ãƒ­ã‚°ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # --- ã‚°ãƒ©ãƒ•æç”» ---
    plt.figure(figsize=(10, 6))
    plt.plot(dates, counts, marker='o', linestyle='-', color='blue')
    plt.title("è‡ªå·±é€²åŒ–å±¥æ­´ï¼ˆè‡ªå·±äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ä»¶æ•°æ¨ç§»ï¼‰")
    plt.xlabel("æ—¥æ™‚")
    plt.ylabel("è‡ªå·±äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ä»¶æ•°")
    plt.grid(True)
    plt.tight_layout()

    # --- ä¿å­˜ ---
    plt.savefig(output_file)
    plt.close()
    print(f"[INFO] é€²åŒ–å±¥æ­´ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")

def verify_predictions(predictions, historical_data, top_k=5, grade_probs=None):
    def check_number_constraints(numbers):
        return (
            len(numbers) == 3 and
            all(0 <= n <= 9 for n in numbers)
        )

    print("[INFO] äºˆæ¸¬å€™è£œã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­...")

    cycle_scores = calculate_number_cycle_score(historical_data)
    valid_predictions = []

    for pred, conf in predictions:
        try:
            numbers = np.sort(np.array(pred if isinstance(pred, (list, np.ndarray)) else pred[0]))
            if check_number_constraints(numbers) and calculate_pattern_score(numbers.tolist()) >= 2:
                avg_cycle = np.mean([cycle_scores.get(n, 999) for n in numbers])
                cycle_score = max(0, 1 - (avg_cycle / 50))
                final_conf = 0.7 * conf + 0.3 * cycle_score
                valid_predictions.append((numbers.tolist(), final_conf))
        except Exception as e:
            print(f"[WARNING] äºˆæ¸¬ãƒ•ã‚£ãƒ«ã‚¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    if not valid_predictions:
        print("[WARNING] æœ‰åŠ¹ãªäºˆæ¸¬ãŒã‚ã‚Šã¾ã›ã‚“")
        return []

    # âœ… PPO / Diffusion ç”±æ¥ã®æ§‹æˆã‚’1çµ„å«ã‚ã‚‹ï¼ˆä¿¡é ¼åº¦ã§åˆ¤å®šï¼‰
    ppo_or_diffusion_found = any(0.90 <= conf <= 0.93 for _, conf in valid_predictions)
    if not ppo_or_diffusion_found:
        fallback_candidate = None
        for pred, conf in valid_predictions:
            if 0.89 <= conf <= 0.94:
                fallback_candidate = (pred, conf)
                print(f"[INFO] PPO/Diffusionä¿è¨¼è£œå®Œ: {pred} (conf={conf:.3f})")
                break
        if fallback_candidate:
            valid_predictions.insert(0, fallback_candidate)
        else:
            print("[WARNING] PPO/Diffusionä¿è¨¼å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

    historical_list = [parse_number_string(x) for x in historical_data["æœ¬æ•°å­—"].tolist()]

    # âœ… ç­‰ç´šæ§‹æˆä¿è¨¼ï¼ˆã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ/ãƒœãƒƒã‚¯ã‚¹ï¼‰
    guaranteed_grade_candidate = None
    for pred, conf in valid_predictions:
        for actual in historical_list[-100:]:
            grade = classify_numbers3_prize(pred, actual)
            if grade in ["ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ", "ãƒœãƒƒã‚¯ã‚¹"]:
                guaranteed_grade_candidate = (pred, conf)
                print(f"[INFO] ç­‰ç´šä¿è¨¼ãƒ‘ã‚¿ãƒ¼ãƒ³ç¢ºä¿: {pred} â†’ {grade}")
                break
        if guaranteed_grade_candidate:
            break

    # ç­‰ç´šä¿è¨¼ã‚‚ãªã‘ã‚Œã°ã€æ“¬ä¼¼ãƒœãƒƒã‚¯ã‚¹æ§‹æˆã§è£œå®Œ
    if not guaranteed_grade_candidate:
        fallback = historical_list[-1]
        alt = list(fallback)
        alt[0] = (alt[0] + 1) % 10
        guaranteed_grade_candidate = (alt, 0.91)
        print(f"[INFO] ç­‰ç´šä¿è¨¼æ§‹æˆã®ãŸã‚ã®è£œå®Œ: {alt}")

    valid_predictions.sort(key=lambda x: x[1], reverse=True)

    # âœ… å¤šæ§˜æ€§ä¿è¨¼ï¼ˆå¥‡å¶æ§‹æˆï¼‰
    def parity_pattern(numbers):
        return tuple(n % 2 for n in numbers)

    diverse_patterns = set()
    selected = [guaranteed_grade_candidate]
    seen = {tuple(guaranteed_grade_candidate[0])}
    diverse_patterns.add(parity_pattern(guaranteed_grade_candidate[0]))

    for pred in valid_predictions:
        key = tuple(pred[0])
        pattern = parity_pattern(pred[0])
        if key not in seen and pattern not in diverse_patterns:
            selected.append(pred)
            seen.add(key)
            diverse_patterns.add(pattern)
        if len(selected) >= top_k:
            break

    print("[INFO] æœ€çµ‚é¸æŠã•ã‚ŒãŸäºˆæ¸¬æ•°:", len(selected))
    return selected

def extract_strong_features(evaluation_df, feature_df):
    """
    éå»äºˆæ¸¬è©•ä¾¡ã¨ç‰¹å¾´é‡ã‚’çµåˆã—ã€ã€Œæœ¬æ•°å­—ä¸€è‡´æ•°ã€ã¨ç›¸é–¢ã®é«˜ã„ç‰¹å¾´é‡ã‚’æŠ½å‡º
    """
    # ğŸ”’ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
    if evaluation_df is None or evaluation_df.empty:
        print("[WARNING] è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€é‡è¦ç‰¹å¾´é‡ã®æŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return []

    if "æŠ½ã›ã‚“æ—¥" not in evaluation_df.columns:
        print("[WARNING] è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã« 'æŠ½ã›ã‚“æ—¥' åˆ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚é‡è¦ç‰¹å¾´é‡ã®æŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return []

    if feature_df is None or feature_df.empty or "æŠ½ã›ã‚“æ—¥" not in feature_df.columns:
        print("[WARNING] ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãŒç„¡åŠ¹ã¾ãŸã¯ 'æŠ½ã›ã‚“æ—¥' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return []

    # ğŸ”§ æ—¥ä»˜å‹ã‚’æ˜ç¤ºçš„ã«æƒãˆã‚‹
    evaluation_df['æŠ½ã›ã‚“æ—¥'] = pd.to_datetime(evaluation_df['æŠ½ã›ã‚“æ—¥'], errors='coerce')
    feature_df['æŠ½ã›ã‚“æ—¥'] = pd.to_datetime(feature_df['æŠ½ã›ã‚“æ—¥'], errors='coerce')

    # â›“ çµåˆ
    merged = evaluation_df.merge(feature_df, on="æŠ½ã›ã‚“æ—¥", how="inner")
    if merged.empty:
        print("[WARNING] è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã¨ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã®çµåˆçµæœãŒç©ºã§ã™ã€‚")
        return []

    # ğŸ“Š ç›¸é–¢è¨ˆç®—
    correlations = {}
    for col in feature_df.columns:
        if col in ["æŠ½ã›ã‚“æ—¥", "æœ¬æ•°å­—", "ãƒœãƒ¼ãƒŠã‚¹æ•°å­—"]:
            continue
        try:
            if not np.issubdtype(merged[col].dtype, np.number):
                continue
            corr = np.corrcoef(merged[col], merged["æœ¬æ•°å­—ä¸€è‡´æ•°"])[0, 1]
            correlations[col] = abs(corr)
        except Exception:
            continue

    # ğŸ” ä¸Šä½5ç‰¹å¾´é‡ã‚’è¿”ã™
    top_features = sorted(correlations.items(), key=lambda x: -x[1])[:5]
    return [f[0] for f in top_features]

def reinforce_features(X, feature_names, important_features, multiplier=1.5):
    """
    æŒ‡å®šã•ã‚ŒãŸé‡è¦ç‰¹å¾´é‡ã‚’å¼·èª¿ï¼ˆå€¤ã‚’å€ç‡ã§å¢—å¼·ï¼‰
    """
    reinforced_X = X.copy()
    for feat in important_features:
        if feat in feature_names:
            idx = feature_names.index(feat)
            reinforced_X[:, idx] *= multiplier
    return reinforced_X

# --- ğŸ”¥ æ–°è¦è¿½åŠ é–¢æ•° ---
def extract_high_match_patterns(dataframe, min_match=2):
    high_match_combos = []
    total = len(dataframe)
    for idx1, row1 in enumerate(dataframe.itertuples(), 1):
        nums1 = set(row1.æœ¬æ•°å­—)
        for idx2 in range(idx1 + 1, total):
            nums2 = set(dataframe.iloc[idx2]['æœ¬æ•°å­—'])
            if len(nums1 & nums2) >= min_match:
                high_match_combos.append(sorted(nums1))
        if idx1 % 50 == 0:
            print(f"[DEBUG] ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒé€²è¡Œä¸­... {idx1}/{total}")
    return high_match_combos

def calculate_number_frequencies(dataframe):
    """éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç•ªå·å‡ºç¾é »åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    all_numbers = [num for nums in dataframe['æœ¬æ•°å­—'] for num in nums]
    freq = pd.Series(all_numbers).value_counts().to_dict()
    return freq

def calculate_number_cycle_score(dataframe):
    """ç•ªå·ã”ã¨ã®å‡ºç¾å‘¨æœŸã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    number_last_seen = {n: None for n in range(0, 10)}
    number_cycle = {n: [] for n in range(0, 10)}

    for i, raw in enumerate(dataframe['æœ¬æ•°å­—']):
        nums = parse_number_string(raw)  # â˜… ã“ã“ã‚’è¿½åŠ ã—ã¦å¤‰æ›
        for n in range(0, 10):
            if n in nums:
                if number_last_seen[n] is not None:
                    cycle = i - number_last_seen[n]
                    number_cycle[n].append(cycle)
                number_last_seen[n] = i

    avg_cycle = {n: np.mean(cycles) if cycles else 999 for n, cycles in number_cycle.items()}
    return avg_cycle

def create_meta_training_data(evaluation_df, feature_df):
    """
    éå»ã®äºˆæ¸¬çµæœã¨ç‰¹å¾´é‡ã‹ã‚‰ã€ãƒ¡ã‚¿å­¦ç¿’ç”¨ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    ç‰¹å¾´é‡: æŠ½ã›ã‚“æ—¥ã®ç‰¹å¾´é‡ç¾¤
    ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: æœ¬æ•°å­—ä¸€è‡´æ•°
    """
    if evaluation_df is None or evaluation_df.empty:
        return None, None

    evaluation_df["æŠ½ã›ã‚“æ—¥"] = pd.to_datetime(evaluation_df["æŠ½ã›ã‚“æ—¥"], errors="coerce")
    feature_df["æŠ½ã›ã‚“æ—¥"] = pd.to_datetime(feature_df["æŠ½ã›ã‚“æ—¥"], errors="coerce")
    
    merged = evaluation_df.merge(feature_df, on="æŠ½ã›ã‚“æ—¥", how="inner")

    target = merged["æœ¬æ•°å­—ä¸€è‡´æ•°"].values
    features = merged.drop(columns=["æŠ½ã›ã‚“æ—¥", "äºˆæ¸¬ç•ªå·", "å½“é¸æœ¬æ•°å­—", "å½“é¸ãƒœãƒ¼ãƒŠã‚¹", "ç­‰ç´š"], errors="ignore")
    features = features.select_dtypes(include=[np.number]).fillna(0)

    return features.values, target

def train_meta_model_maml(evaluation_csv="evaluation_result.csv", feature_df=None):
    from sklearn.linear_model import Ridge
    from sklearn.base import clone

    if not os.path.exists(evaluation_csv):
        print(f"[INFO] {evaluation_csv} ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€Metaãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return None

    try:
        eval_df = pd.read_csv(evaluation_csv)
        if feature_df is None:
            df = pd.read_csv("numbers3.csv")
            df["æŠ½ã›ã‚“æ—¥"] = pd.to_datetime(df["æŠ½ã›ã‚“æ—¥"], errors="coerce")
            feature_df = create_advanced_features(df)

        X_meta, y_meta = create_meta_training_data(eval_df, feature_df)
        if X_meta is None or len(X_meta) == 0:
            print("[ERROR] ãƒ¡ã‚¿å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return None

        # MAMLé¢¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼šæ—¥ã”ã¨ã«ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¦å¹³å‡åŒ–
        task_dates = eval_df["æŠ½ã›ã‚“æ—¥"].unique()
        base_model = Ridge()
        local_models = []

        for date in task_dates:
            sub_eval = eval_df[eval_df["æŠ½ã›ã‚“æ—¥"] == date]
            sub_feat = feature_df[feature_df["æŠ½ã›ã‚“æ—¥"] == date]

            X_task, y_task = create_meta_training_data(sub_eval, sub_feat)
            if X_task is not None and len(X_task) >= 1:
                local_model = clone(base_model)
                local_model.fit(X_task, y_task)
                local_models.append(local_model)

        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ï¼šå„ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡é‡ã¿ï¼ˆç°¡æ˜“å¹³å‡ï¼‰
        if not local_models:
            print("[WARNING] æœ‰åŠ¹ãªãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ãŒä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return None

        final_model = clone(base_model)
        coefs = np.mean([m.coef_ for m in local_models], axis=0)
        intercepts = np.mean([m.intercept_ for m in local_models])
        final_model.coef_ = coefs
        final_model.intercept_ = intercepts
        print("[INFO] MAMLé¢¨ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸ")
        return final_model

    except Exception as e:
        print(f"[ERROR] MAML Metaãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­ã«ä¾‹å¤–ç™ºç”Ÿ: {e}")
        return None
    
def generate_via_diffusion(recent_real_numbers, top_k=5):
    generator = DiffusionNumberGenerator()
    generated = generator.generate(num_samples=100)

    scored = []
    for sample in generated:
        max_sim = max(len(set(sample) & set(real)) for real in recent_real_numbers)
        struct_score = calculate_pattern_score(sample)
        final_score = max_sim + struct_score  # é¡ä¼¼åº¦ + æ§‹é€ ã‚¹ã‚³ã‚¢
        scored.append((sample, final_score))

    scored.sort(key=lambda x: -x[1])
    return [x[0] for x in scored[:top_k]]

def bulk_predict_all_past_draws():
    try:
        df = pd.read_csv("numbers3.csv")
        df["æŠ½ã›ã‚“æ—¥"] = pd.to_datetime(df["æŠ½ã›ã‚“æ—¥"], errors='coerce')
        df = df.sort_values("æŠ½ã›ã‚“æ—¥").reset_index(drop=True)
    except Exception as e:
        print(f"[ERROR] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return

    pred_path = "Numbers3_predictions.csv"
    predicted_dates = set()

    if os.path.exists(pred_path):
        try:
            prev = pd.read_csv(pred_path)
            predicted_dates = set(pd.to_datetime(prev["æŠ½ã›ã‚“æ—¥"], errors='coerce').dt.date.dropna())
        except Exception as e:
            print(f"[WARNING] æ—¢å­˜äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    if not os.path.exists("gpt4numbers.pth") or not os.path.exists("memory_encoder.pth"):
        decoder, encoder = train_gpt3numbers_model_with_memory()
    else:
        decoder = GPT4Numbers()
        decoder.load_state_dict(torch.load("gpt4numbers.pth", map_location=device))
        encoder = MemoryEncoder()
        encoder.load_state_dict(torch.load("memory_encoder.pth", map_location=device))

    decoder.to(device)
    encoder.to(device)
    decoder.eval()
    encoder.eval()

    for i in range(10, len(df)):
        sub_data = df.iloc[:i+1]
        latest_date = sub_data["æŠ½ã›ã‚“æ—¥"].max()

        if latest_date.date() in predicted_dates:
            continue

        actual_numbers = parse_number_string(sub_data.iloc[-1]["æœ¬æ•°å­—"])

        all_groups = {
            "PPO": ppo_multiagent_predict(sub_data),
            "Diffusion": diffusion_generate_predictions(sub_data, 5),
            "Transformer": transformer_generate_predictions(sub_data),
            "GPT": gpt_generate_predictions_with_memory_3(
                decoder, encoder,
                sub_data["æœ¬æ•°å­—"].apply(parse_number_string).tolist(),
                num_samples=5
            )
        }

        all_candidates = []
        for preds in all_groups.values():
            all_candidates.extend(preds)

        verified_predictions = verify_predictions(all_candidates, sub_data)

        if not verified_predictions:
            continue

        result = {
            "æŠ½ã›ã‚“æ—¥": latest_date.strftime("%Y-%m-%d")
        }

        for j, (numbers, conf) in enumerate(verified_predictions[:5]):
            result[f"äºˆæ¸¬{j+1}"] = ",".join(map(str, numbers))
            result[f"ä¿¡é ¼åº¦{j+1}"] = round(conf, 4)
            if 0.90 <= conf <= 0.93:
                result[f"å‡ºåŠ›å…ƒ{j+1}"] = "PPO/Diffusion/GPT"
            elif conf > 0.93:
                result[f"å‡ºåŠ›å…ƒ{j+1}"] = "Transformer"
            else:
                result[f"å‡ºåŠ›å…ƒ{j+1}"] = "Unknown"

        result_df = pd.DataFrame([result])

        if os.path.exists(pred_path):
            try:
                existing = pd.read_csv(pred_path)
                result_df = pd.concat([existing, result_df], ignore_index=True)
            except Exception as e:
                print(f"[WARNING] ä¿å­˜å‰ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")

        result_df.to_csv(pred_path, index=False, encoding="utf-8-sig")
        print(f"[INFO] {latest_date.strftime('%Y-%m-%d')} ã®äºˆæ¸¬ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

        # âœ… çµ±åˆã•ã‚ŒãŸè©•ä¾¡ï¼‹é›†è¨ˆé–¢æ•°ã‚’ã“ã“ã§å®Ÿè¡Œ
        try:
            evaluate_and_summarize_predictions(
                pred_file=pred_path,
                actual_file="numbers3.csv",
                output_csv="evaluation_result.csv",
                output_txt="evaluation_summary.txt"
            )
        except Exception as e:
            print(f"[WARNING] è©•ä¾¡å‡¦ç†ã«å¤±æ•—: {e}")

        predicted_dates.add(latest_date.date())

    print("[INFO] éå»ã™ã¹ã¦ã®äºˆæ¸¬ãƒ»è©•ä¾¡å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    
if not os.path.exists("transformer_model.pth"):
    try:
        df = pd.read_csv("numbers3.csv")
        df["æŠ½ã›ã‚“æ—¥"] = pd.to_datetime(df["æŠ½ã›ã‚“æ—¥"], errors='coerce')
        df = df.sort_values("æŠ½ã›ã‚“æ—¥").reset_index(drop=True)
        train_transformer_with_cycle_attention(df)
    except Exception as e:
        print(f"[ERROR] Transformerå­¦ç¿’å¤±æ•—: {e}")

if __name__ == "__main__":

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    try:
        df = pd.read_csv("numbers3.csv")
        df["æŠ½ã›ã‚“æ—¥"] = pd.to_datetime(df["æŠ½ã›ã‚“æ—¥"], errors='coerce')
        df = df.sort_values("æŠ½ã›ã‚“æ—¥").reset_index(drop=True)
    except Exception as e:
        print(f"[ERROR] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        exit()

    # Diffusionãƒ¢ãƒ‡ãƒ«ãŒãªã‘ã‚Œã°å­¦ç¿’
    if not os.path.exists("diffusion_model.pth"):
        print("[INFO] Diffusionãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
        train_diffusion_model(df, model_path="diffusion_model.pth", epochs=100)

    # Transformerãƒ¢ãƒ‡ãƒ«ãŒãªã‘ã‚Œã°å­¦ç¿’
    if not os.path.exists("transformer_model.pth"):
        print("[INFO] Transformerãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
        train_transformer_with_cycle_attention(df, model_path="transformer_model.pth", epochs=50)

    # ğŸ” ä¸€æ‹¬äºˆæ¸¬ã‚’å®Ÿè¡Œ
    # bulk_predict_all_past_draws()
    main_with_improved_predictions()
    